---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 14"
author: "Helene Behrens, Ellisiv Steen and Johanne Skogvang"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  #html_document
  pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
```

```{r,eval=FALSE,echo=TRUE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("bestglm")# for subset selection with categorical variables
install.packages("glmnet")# for lasso
install.packages("tree") #tree
install.packages("randomForest") #for random forest
install.packages("ElemStatLearn") #dataset in Problem 2
install.packages("ggdendro")
BiocManager::install(c("pheatmap")) #heatmap in Problem 2
```

```{r,eval=FALSE}
install.packages("ggplot2")
install.packages("GGally") # for ggpairs
install.packages("caret") #for confusion matrices
install.packages("pROC") #for ROC curves
install.packages("e1071") # for support vector machines
install.packages("nnet") # for feed forward neural networks
install.packages("MASS")
```

# Problem 1: Regression [6 points]

```{r load diamonds}
all=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/diamond.dd")
dtrain=all$dtrain
dtest=all$dtest
```

**Q1**: Would you choose `price` or `logprice` as response variable? Justify your choice. Next, plot your choice of response pairwise with `carat`, `logcarat`, `color`, `clarity` and `cut`. Comment.
```{r plott}
hist(dtrain$price)
hist(dtrain$logprice)

fit_price = lm(price ~ .-logprice, data=dtrain)
rresprice = rstudent(fit_price)
```

Looking at the histograms of price frequency for the `price` and `logprice` response variable, we see that while the `price` variable have much higher frequenies at lower values, the `logprice` variable is more evenly and symertically distributed for different values. We then want to use `logprice` as our response variable to more easily observe nuances in the data set. 
We can observe the same by looking at Boxcox and residual plots for `price`and `logprice`. 

```{r plott2}
library(ggplot2)
# residuls vs fitted
ggplot(fit_price, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. standardized residuals for price",
       subtitle = deparse(fit_price$call))

library(MASS)
boxcox(fit_price, plotit=TRUE, title = "BoxCox plot for price")

fit_logprice = lm(logprice ~.-price, data=dtrain)
library(ggplot2)
# residuls vs fitted
ggplot(fit_logprice, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. standardized residuals for logprice",
       subtitle = deparse(fit_logprice$call))
library(MASS)
boxcox(fit_logprice, plotit=TRUE)


```
We see that in the BoxCox plots, the lambda value at maximum log-likelihood is closer to one for the logprice than for price. When lambda is close to one it means that one should use the model that is plotted. Also, we want the residuals to be independent of each other, meaning they are randomly spread and that we do not see a pattern in the residual plot. We also we want their variance to be constant. Therefore we choose to use the logprice as responce variable. 

We now plot `logprice` with `carat`, `logcarat`, `color`, `clarity` and `cut`.

```{r }
plot(dtrain$carat, dtrain$logprice, main = 'Carat vs logprice')
plot(dtrain$logcarat, dtrain$logprice, main = 'Logcarat vs logprice')
plot(dtrain$clarity, dtrain$logprice, main = 'Clarity vs logprice')
plot(dtrain$cut, dtrain$logprice, main = 'Cut vs logprice')
```

From both the plot with `carat` and `logcarat` we observe a clear indication that higher carat leads to a higher price, which is expected. The relation between `logcarat` and `logprice` seems to be linear, which indicates that the price grows linearly with the carat. 

The relation between `logprice`and `clarity` is less apparent. Looking at the medians and the mid 50 percentiles, is seems as though the price decreses with better clarity, which is counterintuitive. We do however, observe that many outliers in the most extreme observations, and that the whiskers for most of the clarities cover mostly the entire price range. This indicates that the clarity has little influence on the diamond price. 

Some of the same is observable in the plot between `cut` and `logprice`, where the whiskers of the boxplots covers most of the price range for all cuts. 

We now use the local regression model $\texttt{logprice} = \beta_0 + \beta_1 \texttt{carat}+ \beta_2 \texttt{carat}^2$ weighted by the tricube kernel $K_{i0}$. 

**Q2:** What is the predicted price of a diamond weighting 1 carat. Use the closest 20% of the observations.  
```{r message=FALSE, warning=FALSE}
#fit = loess(dtrain$logprice ~ dtrain$carat, span=.2)
fit = loess(logprice ~ carat, data=dtrain, span=.2 )
10^predict(fit, 1)

```

**Q3:** 
In local regression we have $\hat{f}=\beta_0+\beta_1 x_i+\beta_2x_i^2$, while for KNN-regression this $\hat{f}$ is konstant equal to the mean of the observations in the neighbourhood. Hence, for local regression to be equal to KNN-regression we have $\beta_1 = \beta_2 = 0$. Also, we want $K_{i0}$ equal to one for the observations in the neighbourhood, and zero for the ones outside. 

**Q4:** Describe how you can perform model selection in regression with AIC as criterion.  
When we perform model selection in regression with AIC as the criterion, we perform a best subset selcetion, and choose the set of covariates that produce the lowest AIC value. 
The full procedure when one has a model depending on $k$ covariates is as follows:
1) For every value $i$ from 1 to $k$, all $k \choose j$ models that contain $j$ covariates are fitted. 
2) The model $M_j$ with the lowest SSE value is selected. 
3) From all $M_j$, $j  = 0, ... , k$, where $M_0$ is the model with only the intercept, the model with the lowest AIC is selected. The AIC is defined as

$$\text{AIC} = \frac{\text{SSE}/n}{\hat{\sigma^2}} + 2\frac{p}{n}$$
where $n$ is the number of observations and $p$ is the number of selected covariates in the given model. 

**Q5:** What are the main differences between using AIC for model selection and using cross-validation (with mean squared test error MSE)?

Cross-validation is in general more computationally expensive than model selection with AIC, however, this is rearly considered a problem with today's computers. Cross-validation also measures the test error directly, instead of adjusting the training error for model size, as the AIC does. Finally, cross-validation makes fewer assumptions about the underlying model. Cross-validation does not directly punish models with many covariates; if two models with different complexity produces the same test error, cross-validation will do nothing to choose the less complex model. 

**Q6:** See the code below for performing model selection with `bestglm()` based on AIC. What kind of contrast is used to represent `cut`, `color` and `clarity`? Write down the final best model and explain what you can interpret from the model.    

To represent `cut`, the `fair cut` is removed and used as a reference. This means that all the other covariates linked to how good a cut is has $\beta$ values that represents the relative change from the price that would have been predicted for a `fair cut`. The same holds for the $\beta$ values linked to `color` and `clairity`, where `colorD` and `clarityI1` are used as reference values. 

Looking at the summary of the `best subset model`, one can see that the best model is 
$$ \text{logprice} =\beta_{\text{intercept}} + \beta_{\text{logcarat}} \cdot x_{\text{logcarat}} + \beta_{\text{cutGood}}\cdot x_{\text{cutGood}} + \beta_{\text{cutVeryGood}}\cdot x_{\text{cutVeryGood}} +\beta_{\text{cutPremium}}\cdot x_{\text{cutPremium}} + \beta_{\text{cutIdeal}}\cdot x_{\text{cutIdeal}} \\ 
+ \beta_{\text{colorE}}\cdot x_{\text{colorE}} + \beta_{\text{colorF}}\cdot x_{\text{colorF}} + \beta_{\text{colorG}}\cdot x_{\text{colorG}} + \beta_{\text{colorH}}\cdot x_{\text{colorH}} +  \beta_{\text{colorI}}\cdot x_{\text{colorI}} + \beta_{\text{colorJ}}\cdot x_{\text{colorJ}} \\
+ \beta_{\text{claritySI2}}\cdot x_{\text{claritySI2}} + \beta_{\text{claritySI1}}\cdot x_{\text{claritySI1}} + \beta_{\text{clarityVS2}}\cdot x_{\text{clarityVS2}} + \beta_{\text{clarityVS1}}\cdot x_{\text{clarityVS1}}\\
+ \beta_{\text{clarityVVS2}}\cdot x_{\text{clarityVVS2}} + \beta_{\text{clarityVVS1}}\cdot x_{\text{clarityVVS1}} + \beta_{\text{clarityIF}}\cdot x_{\text{clarityIF}} + \beta_{\text{xx}}\cdot x_{\text{xx}} $$

The worst `cut` is used as a reference, and therefore the $\beta$'s are positive and logprice will increase with a better cut. The same holds for `clarity`, while the best value is used for `color`, and thus the $\beta$'s are negative. One can also see that the depth, table and witdth are not included in this model. This could mean that the shape of the diamond isn't as important as the weight. All of the remainding covariates are significant. 

```{r}

library(bestglm)
ds=as.data.frame(within(dtrain,{
  y=logprice    # setting reponse
  logprice=NULL # not include as covariate
  price=NULL    # not include as covariate
  carat=NULL    # not include as covariate
  }))
fit_bsm=bestglm(Xy=ds, IC="AIC")$BestModel
summary(fit_bsm)

```


**Q7:**  

```{r,eval=FALSE}

pred_bsm = predict(fit_bsm, newdata=dtest)
mse_bsm = mean((dtest$logprice - pred_bsm)^2)
cat("Using the best model, the MSE is calculated to be", mse_bsm)
```

**Q8:** Build a model matrix for the covariates `~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`. What is the dimension of this matrix?  
```{r}
#using the -1 to prevent lasso from removing the indercept.
x_train = model.matrix(logprice~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1, data=dtrain)
cat("The dimension of the model matrix using logcarat, cut, clarity, color, depth, table, xx, yy and zz is: (",dim(x_train)[1], "x", dim(x_train)[2],").")

```

**Q9:** 
We fit a lasso regression to the diamond data with `logprice` as the response and use the model matrix from question 8.
```{r}
y_train = dtrain$logprice
library(glmnet)
set.seed(1)
cv.out = cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out)

best_lambda_lasso = cv.out$lambda.min
best_lambda_lasso

#we fit a model using the lambda found above
fit_lasso = glmnet(x_train,y_train,alpha=1, lambda=best_lambda_lasso) 
coef(fit_lasso)

```
The value for the regularization parameter, $\lambda$, was found using cross validation.

**Q10:** Calculate and report the MSE of the test set (on the scale of `logprice`).  
```{r}
x_test = model.matrix(~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1, data=dtest)
y_test = dtest$logprice

lasso_predictions = predict(fit_lasso, newx = x_test)
lasso_square_errors = ((lasso_predictions-y_test)^2)


mse_lasso=mean(lasso_square_errors)
cat("The MSE of the test set (on the scale of logprice) is calculated to be", mse_lasso)
```

**Q11:** When using a greedy approach one always takes the locally optimal choice, not taking future choices into account. When using a greedy approach to construct a regression tree, one will always make the best splits, meaning one will choose the split that reduces the RSS the most. 

When constructing a regression tree one uses the top-down implementation with the greedy approach. One starts at the top where all observations belong to one single region. Then this region is split into two new branches, choosing the split with the highest reduction in the RSS. The next split in the tree is also the split that reduces the RSS the most, and so it continues until the desired number of regions is reached. All of these regions get one prediction each, which is the mean of the training data that falls into that region. 

**Q12:** A regression tree will be a suitable method for both numerical and categorical covariates; it is when the response is categorical one should use a classification three instead. A regression tree can be used in most cases where normal linear regression can also be used, and it has the benefits that it is easier to interpret and that it can be simplified using bagging and boosting, etc. It can also be added that when constructing a regression tree with numerical covariates, one should choose the splits according to what produces the lowest MSE. 

A regression tree will be a suitable method for both numerical and categorical covariates; it is when the response is categorical one should use a classification three instead. A regression tree can be used in most cses where normal linear regression can also be used, and it has the benefits that it is easier to interpret and that it can be simplified using bagging and boosting, etc. It can also be added that when constructing a regression tree with numerical covariates, one should choose the splits according to what produces the lowest MSE. 

**Q13:** Fit a (full) regression tree to the diamond data with `logprice` as the response (and the same covariates as for c and d), and plot the result. Comment briefly on you findings. 

From the drawing of the regression tree below, we see that it only sorts on the lenght and width of the diamond, which should indicate that these are the most important covariates when classifying the price of the diamond. However, the best subset selection method for instance indicates that many more of the parameters is more or equally significant to the lenght and that the width is in fact statistically insignificant. This means that we can expect some errors when prediciting with this model. 

```{r}
library(tree)
tree.logprice = tree(logprice~logcarat+cut+clarity+color+depth+table+xx+yy+zz, data=dtrain)
plot(tree.logprice)
text(tree.logprice, pretty=1)

```

**Q14:**    
```{r}
tree_pred = predict(tree.logprice, newdata=dtest)
mse_tree=mean((tree_pred - dtest$logprice)^2)
mse_tree

```
The MSE of the regression tree is $0.01715548$.


**Q15:**  When we average over different observations, the variance is typically lower than for one single observation. One can use the bootstrap to find $B$ repeated samples from the training data set, train each of the $B$ sets, and then average on the trained models. This is called bootstrap aggregation, or bagging, and will reduce the variance of the model. 

Bagging can typically be used in regression trees, since using one single tree will result in high variance. Then we get $B$ bootstrapped trees that we use for the final result by taking the average. Random forest is similar to bagging, but instead of using each of the predictors when choosing a split, a smaller sample of the predictors is used. This is called decorrelation of the trees, and will lead to an even lower variance.


**Q16:** In random forest one will have to choose the number of trees, $B$. A large number of trees will not lead to overfitting, and therefore $B$ is chosen sufficiently large, so that the error rate stabilizes. 

The other parameter that needs to be set is the number of predictors used in each split, $m$. One typically chooses $m \approx \sqrt{p}$, where $p$ is the number of predictors. Then the algorithm is no longer allowed to choose splits based on the majority of the predictors. Then if one predictor is particulary strong, it will not be at the top of the trees as often as it would if bagging was used. Hence, the other predictors are chosen more often, and the trees are thus decorrelated. 

**Q17:** The main difference between random forest and boosting is that in random forest, each tree is constructed based on the bootstrapped sample from the training data, while in boosting, the trees are build using the residuals, meaning that each tree is build using information from the previously built trees. This makes boosting an approach that learns slowly, which in general usually means that it performs well. 

**Q18:** When using random forest, one usually choose $m \approx \sqrt{p}$ as stated above, which in this case means that $ m \approx \sqrt{9} = 3$. Common choices for the number of trees are $B=300$ and $B=500$, and here $B=300$ should be sufficient. 

```{r random forest 1}
set.seed(300)
library(randomForest)
rf.logprice=randomForest(logprice~logcarat+cut+clarity+color+depth+table+xx+yy+zz, data=dtrain, mtry=3, ntree=300, importance=TRUE)
plot(rf.logprice)
```

**Q19:** We make a variable importance plot to study which of the covariates that are most important when explaining the price of a diamond. 
```{r}
varImpPlot(rf.logprice, type = 2)
rf.pred=predict(rf.logprice, newdata=dtest)
mse_rf=mean((rf.pred-dtest$logprice)^2)
cat("The MSE of the random forest:",mse_rf)

```

**Q20:** Finally, we compare the results from c (subset selection), d (lasso), e (tree) and f (random forest): Which method has given the best performance on the test set and which method has given you the best insight into the relationship between the price and the covariates?
```{r}
library(utils)
library(pracma)

pred_bsm = predict(fit_bsm, newdata=dtest)
mse_bsm = mean((dtest$logprice - pred_bsm)^2)

mses=c(mse_bsm, mse_lasso, mse_tree, mse_rf)
names=c("Subset selection", "Lasso", "Tree", "Random forest")
mse_range = linspace(0,0.2,101)
plot(mses, axes=FALSE, xlab = 'Methods', ylab = 'MSE')
axis(1, at=1:4, lab=names)
axis(2,mse_range)
box()

print("the MSEs: ")
for (i in c(1,2,3,4)){
  print(paste(names[i], sep = ": ", round(mses[i], 4)))
}

```

We see that the regression tree has a substantially higher MSE than the rest, and that the random forest has the best MSE. This also indicats that the regression tree will provide us with the least information about the relation between the covariates and the price. This coincides with what we have previously observed, where the regression tree only takes two covariates into account in the prediction. 

Even though the remaining models have approximately equal MSE, some of them gives easier insight into the relationship between the covariates and the price. The random forest method for instance, operates as a black box, and thus gives little information about the effect of the different covariates on the response value. 

In both the subset selection method and the lasso method, the value of the regression parameters and which covariates are included in the resulting model indicates how the different covariates influences the response. The subset selection model gives a clear result of which covariates are considered significant, which provides an easily interpretable model. For instance, one can quickly conclude on which covariates does not have a high influence on the price by looking at the result from the subset selection. 

The lasso regression, in addition to removing the completely insignificant covariates, also shrinks the value of the coefficiants of the less significant covariates. This means that one can get an impression of what covariate has the most influence on the response variable as well. In our case, we see that the logcarat coeffeicient is substantially higher than the rest. This coincides well with our initial interpretation that the carat is the one covariate that influences the price the most. However, one should notice that this method of interpreting the model is not always correct, as regression coefficient also takes the value size of the covariates into account, meaning that covariates that contribute with lower values might have coefficients of higher values without being more significant.

# Problem 2: Unsupervised learning [3 points]

**Q21:** The principal component score is a measurement of how important a given set of observations is for a principal component. The principal components are the eigenvectors of the covariance matrix ${\hat {\bf R}}$, and thus a principal component score for a set of observations is the scaled observations multiplied with a principal component.
$$\text{score}_i = \sum_{j=1}^{p}x_{ij}\cdot(PC_{ij})$$

```{r}
library(ElemStatLearn)
X=t(nci) #n times p matrix
ngroups=length(table(rownames(X)))
cols=rainbow(ngroups)
cols[c(4,5,7,8,19)] = "black"
pch.vec = rep(4,14)
pch.vec[c(4,5,7,8,14)] = 15:19

colsvsnames=cbind(cols,sort(unique(rownames(X))))
colsamples=cols[match(rownames(X),colsvsnames[,2])] 
pchvsnames=cbind(pch.vec,sort(unique(rownames(X))))
pchsamples=pch.vec[match(rownames(X),pchvsnames[,2])]

Z=scale(X)

pca=prcomp(Z)
plot(pca$x[,1],pca$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)

plot(1:dim(X)[2],pca$rotation[,1],type="l",xlab="genes",ylab="weight",main="First eigenvector")

```


**Q22:** The plot "First eigenvector" shows the weights of the first eigenvector of $\hat{\bf{R}}$ which is also the first principal component. The reason why there are only $64$ principal components is that there can not be more principal components than the rank of $\bf{X}$. This is because all the principal components are linearly independent, and thus the number of principal components (which is the eigenvectors of $\hat{\bf{R}}$) is bounded by the rank of $\bf{X}$ which is maximally $n - 1=64$ (reduces to $n-1$ because of the centering)). The rank is actually $min(n-1, p)$, but because p is very large in our case it is obviously $n-1$. As seen from **Q21**, the score is directly related to a given principal component, so the number of component scores is always the same as the number of principle components.

**Q23:** The proportion of variance explained by a single principle component is calculated as 
$$\text{PVE}=\frac{\text{variance explained by }m\text{th component}}{\text{total variance}} = \frac{\frac{1}{n}\sum_{i=1}^{n}(\sum_{j = 1}^{p} \phi_{jm}z_{ij})^2}{\frac{1}{n}\sum_{j=1}^{p}\sum_{i=1}^{n}z_{ij}}.$$
From this formula it is also easy to see that the total variance is `sum(pca$sdev^2)=p`. The total variance defined above is the same as the sum of the variances of the columns of $\bf{Z}$, which is $p$ since they are standardized, and thus every column has variance $1^2=1$. An easier way of computing the proportion of variance is to use the eigenvalues corresponding to the principle components. It can be shown that $\text{Var}(\phi_j) = \lambda_j$ and thus $$\text{PVE}=\frac{\lambda_m}{\sum_{j=1}^{p}\lambda_j}$$

```{r proportion of variance plot}
pve=100*pca$sdev^2/sum(pca$sdev^2)
plot(cumsum(pve), type = "o", ylab = "Cumulative PVE", xlab = "Principal component", col="brown3")

```

We can see from the plot that it takes about 30 principal components to explain 80% of the variance, and we check for the different number of principal components to check this and find the exact answer.

```{r proportion of variance}

totvar = sum(pca$sdev^2)
pve32=sum(pca$sdev[1:32]^2)/totvar

pve31 = sum(pca$sdev[1:31]^2)/totvar

cat("The first 31 principal components explains", pve31 * 100, "% of the variance and 32 principal components explains", pve32 * 100, "% of the variance. Thus we need 32 principal components to explain 80% of the variance in the data.")
```

**Q24**: From the plot above, plotting PC1 agianst PC2, the most obvious clustering is the blue clustering, where the genes are from `MELANOMA` cancer tumors. The green `LEUKEMIA` genes are also easy to seperate from the others, and so are genes from `COLON` cancer tumors. We observe that the genes from other cancer types are mostly located in the left upmost corner and are also, to some degree, clustered. The exeption is the genes from the breast cancer tumor, which are not clustered at all and are spread around the entire plot. 
  It is known that the `K562` samples are leukemia cells and we can see from the plot that they are clustered together with the other leukemia samples. It is harder to say something about the breast cancer samples, `MCF7`, because the other breast cancer samples are also spread widely around. The unknown sample lays in the upper left corner, so it could belong to any of those groups. However, the most likely cancer types are `NSCLC` or `OVARIAN` because the closest points are of those types.

We plot a couple of other projections of the data onto the principal components to see if we can find other interesting relationships. It is mostly interesting to look at the first principal comonents, because those contain the most variance. It could have been interesting to find plots where the breast cancer cells are clustered together or find a relation for the unknown sample, but this turned out to be difficult. Below two other pairs of principal components are plotted together. In the first plot we see PC1 agianst PC3 and here we also see that green and yellow are clustered. Again, the remaining cancer types are slightly clustered and it is hard to find a relation between the cases of breast cancer. However we see that the `MFC7` samples of breast cancer are slightly more separated from the cluster of `COLON` cases and are still closely associated with a breast cancer case. This is shown even more clearly by plotting PC3 against PC4, as shown below. The `UNKNOWN` sample was hard to classify for all combinations we tried, and because of this we conclude that it probably belongs to one of the cancer types that are difficult to separate from the others. Our strongest hypothesis is that it belongs to the `NSCLC` type.

```{r}
plot(pca$x[,1],pca$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)

plot(pca$x[,1],pca$x[,3],xlab="PC1",ylab="PC3",pch=pchsamples,col=colsamples)
legend("bottomleft",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)

plot(pca$x[,3],pca$x[,4],xlab="PC3",ylab="PC4",pch=pchsamples,col=colsamples)
legend("bottomleft",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
```


**Q25:**: A hierarchical clustering algorithm starts with dividing all elements into its own cluster and iteratively merging the closest clusters to bigger clusters. It then has to be decided how one can say which clusters are closer than other when the dimensions grow larger than two. The positions of the points are defined by a vector of the values of the covariates, and thus we can use a norm to express the distance between two points. The most common norm is the 2-norm (also called Eucledian distance). For deciding the distance between two clusters there are also several choices. There are three common methods, where the average linkage is calculating the mean position of each cluster and calculating the distance between these two points.

**Q26:**: 

```{r q26}
library(ggplot2)
library(ggdendro)

ggdendrogram(hclust(dist(Z), method = "average"))+labs(title="Hierarchical clustering")

```
From the Hierarchical clustering performed above we can observe how the algorithm places the samples compared to the well-known samples. We start by looking at the `K562` samples and see that they are clustered together with `LEUKEMIA` as expected. The Hierarchical clustering, as we did in **Q24**, struggles with clustering the `BREAST` samples together, but in the same way we saw it classifies the `MCF7` samples closest to two `BREAST` samples. The `UNKNOWN` sample is by the algorithm nearest to `OVERIAN` samples and not `NSCLC` as we suspected. But we do get confirmed that `NSCLC` is somewhat randomly spread, so the basis of our suspicion is well-grounded, we just did not belive that the sample was closer to the `OVERIAN` samples.

**Q27:**: In the plot below it is preformed hierarchical clustering based on the principal components. This means that the positions of the points are based on the values when the vector are multiplied with the principal components. This means that the clusterings are based on vectors of length 64 in stead of 6830. One entry of the heat map grid shows the value of a PCA multiplied with the gene composition vector of a sample. The horizontal axis is the PCAs sorted by the amount of variation explained in the data, from most to less. On the vertical axis the covariates are sorted by the hierchial clustering algorithm.

```{r}
library(pheatmap)
npcs=64 
pheatmap(pca$x[,1:npcs],scale="none",cluster_col=FALSE,cluster_row=TRUE,clustering_distance_rows = "euclidean",clustering_method="average",fontsize_row=5,fontsize_col=5)
```




# Problem 3: Flying solo with diabetes data [6 points]

```{r}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest
```

**Q28:** Start by getting to know the _training data_, by producing summaries and plots. Write a few sentences about what you observe and include your top 3 informative plots and/or outputs.
```{r}

flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest

library(GGally)
summary(ctrain) #gir info om vanlige verdier for kovariatene
str(ctrain) #en kompkt måte å se hvordan datasettet ser ut 
head(ctrain) #viser den første delen av dataframen 
ctrain$diabetes = as.factor(ctrain$diabetes)
ggpairs(ctrain, aes(color=diabetes))
```


Include pair plot to investigate for collinearity. 

**Q29:** Use different methods to analyse the data. In particular use 

* one method from Module 4: Classification   
We choose logistic regression since we only have two responce classes. When using logistic regression, the model is based on probabilities that $Y=1$. That means the probability that a person has diabetes. In this case, this probability is calculated by 
$$ p(x_i)= p_i = \frac{e^{{\beta}_{intercept} + {\beta}_{npreg}x_{npreg,i} + {\beta}_{glu}x_{glu,i} + {\beta}_{bp}x_{bp,i} + {\beta}_{skin}x_{skin,i} + {\beta}_{bmi}x_{bmi,i} + {\beta}_{ped}x_{ped,i} + {\beta}_{age}x_{age,i}}}
{1+e^{{\beta}_{intercept} + {\beta}_{npreg}x_{npreg,i} + {\beta}_{glu}x_{glu,i} + {\beta}_{bp}x_{bp,i} + {\beta}_{skin}x_{skin,i} + {\beta}_{bmi}x_{bmi,i} + {\beta}_{ped}x_{ped,i} + {\beta}_{age}x_{age,i}}}.$$ 
One can also write this as 
$$ log \frac{p_i}{1-p_i} = {{\beta}_{intercept} + {\beta}_{npreg}x_{npreg,i} + {\beta}_{glu}x_{glu,i} + {\beta}_{bp}x_{bp,i} + {\beta}_{skin}x_{skin,i} + {\beta}_{bmi}x_{bmi,i} + {\beta}_{ped}x_{ped,i} + {\beta}_{age}x_{age,i}}.$$

These coefficients are found using maximun likelihood estimation. In logistic regression we assume that the observations to be independent of each other, and the variables should not be to highly correlated with each other. As seen when plotting the variables with each other, one can observe some mutlicollinearity between `skin` and `bmi` as well as between `npreg` and `age`. The log-odds is the left side of the last equation. We require little or no linearity between the log-odds and the independent variables, and we also assume that we have a large sample size.    

Fitting the model:
```{r logistic regression}
lrtrain=ctrain
lrtest=ctest
library(glmnet)
diab.glm = glm(diabetes ~ ., data = ctrain, family = "binomial")
summary(diab.glm)
```

If we look at the summary of the model, we see that there are three significate covariates woth p-value less than $0.05$. These are `glu`, `bmi` and `ped`, which in fact all have p-values less than $0.01$. The covariates `bp` and `skin` have rather high p-values, which means that they may not be that important in the model. 



Evaluation of the model:
```{r log reg eval}
diab.glm_prob=predict(diab.glm, newdata=lrtest, type="response")
diab.glm_pred=ifelse(diab.glm_prob > 0.5, 1, 0)
conf_test=table(diab.glm_pred, lrtest$diabetes)
conf_test
misclass_test = (232-sum(diag(conf_test)))/232
misclass_test
```


If one appiles this model to the test data one can find the proporsion of mistakes that are made, called the misclassification test error rate. The misclassification test error rate is thus
$$Test\,\, error = \frac{29+18}{232} = 0.2025862. $$   


Finding ROC-curve and AUC
```{r}
library(pROC)
par(pty='s')
glm_ROC = roc(lrtest[ ,1], diab.glm_prob, plot=TRUE, legacy.axces=TRUE)
glm_ROC
```

The area under the ROC curve, hence the AUC, is calcualed to be 0.8451







* one method from Module 8: Trees (and forests)

We choose to use bagging to analyse the data. When using bagging one constucts bootstrap samples, and here we make $500$ samples. A classification tree is made from each sample, $\hat{f}^{*b}$. $b=1...B$, where $B$ s the number of bootstrap samples. Then one avareage over these predictions. Each of these classification trees will have high variance and low bias, but when we average over the trees, the variance decreases. When we use bagging, we assume that the observations are independent.     

When using bagging instead of one simple classification tree, the variance is decreased, but this is on the cost of interpretability. When you average over multiple trees, you loose som of the interpretability that we find in classification trees. However it is possible to make varibale importance plots. Here the predictors are sorted based on their importance, where the more important predictors for either increase in MSE of node purity are placed further up than the less important ones. 

```{r bagging}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest

library(randomForest)
set.seed(2)
diab.bagging=randomForest(diabetes ~ ., data = ctrain, mtry=7, ntree=500, importance=TRUE)
plot(diab.bagging)
summary(diab.bagging)

```
Her må du skrive hva du får ut av denne mdoellen:
The number of bootstrap samples, $B$, is chosen such that the error has stabilised, and is often chosen to $300$ or $500$. We se that the error is clearly stabilised at $B=500$, and choose that to be the number of bootstrap samples. 

```{r}
varImpPlot(diab.bagging)

```
In the variance importance plot we see that `glu` is the most important covariate both when it comes to increase in MSE when assigning random variables instead of the covariate and in increase of the Gini index when using thw covariate. One can also see that `npreg` and `bp` are the least important variables. 

Evaluation of the model:

Misclassification rate for the test data
```{r}
diab.bag_prob=predict(diab.bagging, newdata=ctest, type="response")
diab.bag_pred=ifelse(diab.bag_prob>0.5, 1, 0)
conf_test_bagging=table(diab.bag_pred, ctest$diabetes)
conf_test_bagging
misclass_test_bagging = (232-sum(diag(conf_test_bagging)))/232
misclass_test_bagging 

```


ROC curve and AUC
```{r ROC bagging}
library(pROC)
par(pty='s')
bagging_ROC=roc(ctest[ ,1], diab.bag_prob, plot=TRUE, legacy.axes=TRUE)
bagging_ROC

```
The $AUC$ is $0.8288$. 

* one method from Module 9: Support vector machines

We choose the support vector machines method, with a radial kernel, the cost of violating the margin $= 10$, and $\gamma = 0.001$. The radial kernel is chosen because it is efficient and helps avoid overfitting of the data. The radial kernel is defined as 
$$ K(\textbf{x}_i, \textbf{x}_i') = \text{exp}(- \gamma \sum_{j = 1}^{p}(x_{ij} - x_{i'j})^2),  $$ 
where $\gamma$ is a tuning parameter and $p$ is the number of covariates. We find the optimal values for the cost of violating the margin and $\gamma$ by the function tune() in R. This results in a model where the width of the margin is $0.1$, as seen from the summary of the selected model.  
 
We notice that the model uses a quite large number of support vectors, 188 out of 300 possible. This could give us suspicions that the model is overfitted, but since the misclassification error is still rather low, it does not seem to be a substantial problem. 
 
```{r svm}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest
svmtrain=ctrain
svmtest=ctest

library(e1071)
set.seed(2)
CV_radial = tune(svm, factor(diabetes) ~ ., data=ctrain, kernel="radial", ranges=list(gamma=c(10^-5, 10^-4,0.001,0.01,0.1,1,2,3,4,5), cost=c(0.001, 0.01, 0.1, 1, 5, 10, 20,40,80,100,120,140,160)))
svm_best_model=CV_radial$best.model
summary(svm_best_model)

#Finding miclassification rate for the test set:
svm_pred_test=predict(svm_best_model, newdata=svmtest, type="response")
conf_test_svm=table(svm_pred_test, ctest$diabetes)
conf_test_svm
misclas_test_svm=(232-sum(diag(conf_test_svm)))/232
cat("Misclassification rate for the test data:", misclas_test_svm)
```


* one method from Module 11: Neural networks
We now use a machine learning technique based on neural networks. Because the application is classification with only two classes we use a network with only one output node. The input layer consists of one node for each covariate which gives an input layer with a total of 7 nodes. We use a method in R called `nnet` which uses one hidden layer where the number of hidden nodes can be varied to fit our problem. To see what works best in our case we will use cross validation to find the best choice for this tuning parameter. 

```{r neural networks}
#loading again because the data has to be unchanged
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest

nntrain <- ctrain
nntest <- ctest

mean <- apply(ctrain, 2, mean)
mean[1] = 0
std <- apply(ctrain, 2, sd)
std[1] = 1
nn_train <- scale(nntrain, center=mean, scale=std)
nn_test <- scale(nntest, center=mean, scale=std)

nn_train.df <- as.data.frame(nn_train)
nn_test.df <- as.data.frame(nn_test)

library(nnet)
library(NeuralNetTools)
library(pROC)

#Cross validation to find best size of hidden layer
set.seed(2)

cv_nn = tune(nnet, diabetes ~ ., data=nn_train.df, ranges=list(size=1:10), linout=FALSE, entropy=TRUE)

nn_best_model=cv_nn$best.model

nn_prob_test=predict(nn_best_model, newdata=nn_test) #jeg får ikke til , type="response"
nn_prob_test
summary(nn_prob_test)
nn_pred_test=ifelse(nn_prob_test>0.5, 1, 0)
conf_test_nn=table(nn_pred_test, ctest$diabetes)
conf_test_nn
misclas_test_nn=(length(ctest[, 1])-sum(diag(conf_test_nn)))/length(ctest[, 1])
cat("Misclassification rate for the test data: ", misclas_test_nn)

roc_curve = roc(response=ctest[,1], predictor=nn_prob_test, legacy.axes=TRUE)

ggroc(roc_curve) + ggtitle("ROC curve for neural network model")

```


For each method you 

* clearly write out the model and model assumptions for the method
* explain how any tuning parameters are chosen or model selection is performed
* report (any) insight into the interpretation of the fitted model
* evaluate the model using the test data, and report misclassifiation rate (cut-off 0.5 on probability) and plot ROC-curves and give the AUC (for method where class probabilities are given).

**Q30:** Conclude with choosing a winning method, and explain why you mean that this is the winning method.
