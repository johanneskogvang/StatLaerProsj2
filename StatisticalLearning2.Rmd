---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 14"
author: "Helene Behrens, Ellisiv Steen and Johanne Skogvang"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  #html_document
  pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
```

```{r,eval=FALSE,echo=TRUE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("bestglm")# for subset selection with categorical variables
install.packages("glmnet")# for lasso
install.packages("tree") #tree
install.packages("randomForest") #for random forest
install.packages("ElemStatLearn") #dataset in Problem 2
install.packages("ggdendro")
install.packages("NeuralNetTools")
BiocManager::install(c("pheatmap")) #heatmap in Problem 2
```

```{r,eval=FALSE}
install.packages("ggplot2")
install.packages("GGally") # for ggpairs
install.packages("caret") #for confusion matrices
install.packages("pROC") #for ROC curves
install.packages("e1071") # for support vector machines
install.packages("nnet") # for feed forward neural networks
install.packages("MASS")
```

# Problem 1: Regression [6 points]

```{r load diamonds}
all=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/diamond.dd")
dtrain=all$dtrain
dtest=all$dtest
```

**Q1**: When getting to know the data we have to know if the response variable should be transformed to be as meaningful as possible. We produce some plots to get to know the behaviour of the response variable.
```{r plott}
hist(dtrain$price)
hist(dtrain$logprice)

fit_price = lm(price ~ .-logprice, data=dtrain)
rresprice = rstudent(fit_price)
```

Looking at the histograms of price frequency for the `price` and `logprice` response variable, we see that while the `price` variable have much higher frequenies at lower values, the `logprice` variable is more evenly and symertically distributed for different values. We then want to use `logprice` as our response variable to more easily observe nuances in the data set. 
We can observe the same by looking at Boxcox and residual plots for `price`and `logprice`, as seen below.  

```{r plott2}
library(ggplot2)
# residuls vs fitted
ggplot(fit_price, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. standardized residuals for price",
       subtitle = deparse(fit_price$call))

library(MASS)
boxcox(fit_price, plotit=TRUE, title = "BoxCox plot for price")

fit_logprice = lm(logprice ~.-price, data=dtrain)
library(ggplot2)
# residuls vs fitted
ggplot(fit_logprice, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. standardized residuals for logprice",
       subtitle = deparse(fit_logprice$call))
library(MASS)
boxcox(fit_logprice, plotit=TRUE)


```
We see that in the BoxCox plots, the lambda value at maximum log-likelihood is closer to one for the logprice than for price. When lambda is close to one it means that one should use the model that is plotted. Also, we want the residuals to be independent of each other, meaning they are randomly spread and that we do not see a pattern in the residual plot. We also we want their variance to be constant. Therefore we choose to use the logprice as responce variable. 

We now plot `logprice` with `carat`, `logcarat`, `color`, `clarity` and `cut`.

```{r }
plot(dtrain$carat, dtrain$logprice, main = 'Carat vs logprice')
plot(dtrain$logcarat, dtrain$logprice, main = 'Logcarat vs logprice')
plot(dtrain$clarity, dtrain$logprice, main = 'Clarity vs logprice')
plot(dtrain$cut, dtrain$logprice, main = 'Cut vs logprice')
```

From both the plot with `carat` and `logcarat` we observe a clear indication that higher carat leads to a higher price, which is expected. The relation between `logcarat` and `logprice` seems to be linear. 

The relation between `logprice`and `clarity` is less apparent. Looking at the medians and the mid 50 percentiles, is seems as though the price decreases with better clarity, which is counterintuitive. We do however observe many outliers in the most extreme observations, and that the whiskers for most of the clarities cover mostly the entire price range. This indicates that the clarity has little influence on the diamond price. 

Some of the same is observable in the plot between `cut` and `logprice`, where the whiskersof the boxplots covers most of the price range for all cuts. 

Use the local regression model $\texttt{logprice} = \beta_0 + \beta_1 \texttt{carat}+ \beta_2 \texttt{carat}^2$ weighted by the tricube kernel $K_{i0}$. 

**Q2:** 
```{r message=FALSE, warning=FALSE}
fit = loess(logprice ~ carat, data=dtrain, span=.2 )
cat("The price of a diamond weighing 1 carat is",10^predict(fit, 1),", when we use the closest 20%")

```

**Q3:** 
In local regression we have $\hat{f}=\beta_0+\beta_1 x_i+\beta_2x_i^2$, while for KNN-regression this $\hat{f}$ is constant equal to the mean of the observations in the neighbourhood. Hence, for local regression to be equal to KNN-regression we have $\beta_1 = \beta_2 = 0$. Also, we want $K_{i0}$ equal to one for the observations in the neighbourhood, and zero for the ones outside. 

**Q4:**   
When we perform model selection in regression with AIC as the criterion, we perform a best subset selcetion, and choose the set of covariates that produce the lowest AIC value. 
The full procedure when one has a model depending on $k$ covariates is as follows:
1) For every value $i$ from $1$ to $k$, all $k \choose j$ models that contain $j$ covariates are fitted. 
2) The model $M_j$ with the lowest SSE value is selected. 
3) From all $M_j$, $j  = 0, ... , k$, where $M_0$ is the model with only the intercept, the model with the lowest AIC is selected. The AIC is defined as

$$\text{AIC} = \frac{\text{SSE}/n}{\hat{\sigma^2}} + 2\frac{p}{n}$$
where $n$ is the number of observations and $p$ is the number of selected covariates in the given model. 

**Q5:** The main difference between cross-validation and AIC is that cross-validation is in general more computationally expensive than model selection with AIC, however, this is rearly considered a problem with today's computers. Cross-validation also measures the test error directly, instead of adjusting the training error for model size, as the AIC does. Finally, cross-validation makes fewer assumptions about the underlying model. Cross-validation does not directly punish models with many covariates; if two models with different complexity produces the same test error, cross-validation will do nothing to choose the less complex model. 

**Q6:** To represent `cut`, the `fair cut` is removed and used as a reference. This means that all the other covariates linked to how good a cut is has $\beta$ values that represents the relative change from the price that would have been predicted for a `fair cut`. The same holds for the $\beta$ values linked to `color` and `clairity`, where `colorD` and `clarityI1` are used as reference values. 

Looking at the summary of the `best subset model`, one can see that the best model is 
$$ \text{logprice} =\beta_{\text{intercept}} + \beta_{\text{logcarat}} \cdot x_{\text{logcarat}} + \beta_{\text{cutGood}}\cdot x_{\text{cutGood}} + \beta_{\text{cutVeryGood}}\cdot x_{\text{cutVeryGood}} +\beta_{\text{cutPremium}}\cdot x_{\text{cutPremium}} + \beta_{\text{cutIdeal}}\cdot x_{\text{cutIdeal}} \\ 
+ \beta_{\text{colorE}}\cdot x_{\text{colorE}} + \beta_{\text{colorF}}\cdot x_{\text{colorF}} + \beta_{\text{colorG}}\cdot x_{\text{colorG}} + \beta_{\text{colorH}}\cdot x_{\text{colorH}} +  \beta_{\text{colorI}}\cdot x_{\text{colorI}} + \beta_{\text{colorJ}}\cdot x_{\text{colorJ}} \\
+ \beta_{\text{claritySI2}}\cdot x_{\text{claritySI2}} + \beta_{\text{claritySI1}}\cdot x_{\text{claritySI1}} + \beta_{\text{clarityVS2}}\cdot x_{\text{clarityVS2}} + \beta_{\text{clarityVS1}}\cdot x_{\text{clarityVS1}}\\
+ \beta_{\text{clarityVVS2}}\cdot x_{\text{clarityVVS2}} + \beta_{\text{clarityVVS1}}\cdot x_{\text{clarityVVS1}} + \beta_{\text{clarityIF}}\cdot x_{\text{clarityIF}} + \beta_{\text{xx}}\cdot x_{\text{xx}} $$

The worst `cut` is used as a reference, and therefore the $\beta$'s are positive and logprice will increase with a better cut. The same holds for `clarity`, while the best value is used for `color`, and thus the $\beta$'s are negative. One can also see that the depth, table and witdth are not included in this model. This could mean that the shape of the diamond isn't as important as the weight. All of the remainding covariates are significant. 

```{r}

library(bestglm)
ds=as.data.frame(within(dtrain,{
  y=logprice    # setting reponse
  logprice=NULL # not include as covariate
  price=NULL    # not include as covariate
  carat=NULL    # not include as covariate
  }))
fit_bsm=bestglm(Xy=ds, IC="AIC")$BestModel
summary(fit_bsm)

```


**Q7:**  

```{r,eval=FALSE}

pred_bsm = predict(fit_bsm, newdata=dtest)
mse_bsm = mean((dtest$logprice - pred_bsm)^2)
cat("Using the best model, the MSE is calculated to be", mse_bsm)
```

**Q8:** We build a model matrix for the covariates `~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`.
```{r}
x_train = model.matrix(logprice~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1, data=dtrain)
cat("The dimension of the model matrix using logcarat, cut, clarity, color, depth, table, xx, yy and zz is: (",dim(x_train)[1], "x", dim(x_train)[2],").")

```

**Q9:** 
We fit a lasso regression to the diamond data with `logprice` as the response and use the model matrix from question 8.
```{r}
y_train = dtrain$logprice
library(glmnet)
set.seed(1)
cv.out = cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out)

best_lambda_lasso = cv.out$lambda.min
best_lambda_lasso

#we fit a model using the lambda found above
fit_lasso = glmnet(x_train,y_train,alpha=1, lambda=best_lambda_lasso) 
coef(fit_lasso)

```
The value for the regularization parameter, $\lambda$, was found using cross validation.

**Q10:** 
```{r}
x_test = model.matrix(~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1, data=dtest)
y_test = dtest$logprice

lasso_predictions = predict(fit_lasso, newx = x_test)
lasso_square_errors = ((lasso_predictions-y_test)^2)


mse_lasso=mean(lasso_square_errors)
cat("The MSE of the test set (on the scale of logprice) is calculated to be", mse_lasso)
```

**Q11:** When using a greedy approach one always takes the locally optimal choice, not taking future choices into account. When using this approach to construct a regression tree, one will always make the locally best splits, meaning one will choose the split that reduces the RSS the most, not considering possible future splits. 

When constructing a regression tree one uses a top-down implementation with the greedy approach. One starts at the top where all observations belong to one single region. Then this region is split into two new branches, choosing the split with the highest reduction in the RSS. The next split in the tree is also the split that reduces the RSS the most, and so it continues until the desired number of regions is reached. All of these regions get one prediction each, which is the mean of the training data that falls into that region. 

**Q12:** A regression tree will be a suitable method for both numerical and categorical covariates; it is when the response is categorical one should use a classification three instead. A regression tree has the benefits that it is easy to interpret and that it can be improved using bagging and boosting, etc. It can also be added that when constructing a regression tree with numerical covariates, one should choose the splits according to what produces the lowest MSE. 

A regression tree will be a suitable method for both numerical and categorical covariates; it is when the response is categorical one should use a classification three instead. A regression tree can be used in most cses where normal linear regression can also be used, and it has the benefits that it is easier to interpret and that it can be simplified using bagging and boosting, etc. It can also be added that when constructing a regression tree with numerical covariates, one should choose the splits according to what produces the lowest MSE. 
**Q13:** We have fitted a regression tree to the diamond data with `logprice` as the response below. From the drawing of the regression tree, we see that it only sorts on the lenght and width of the diamond, which should indicate that these are the most important covariates when classifying the price of the diamond. However, the best subset selection method for instance indicates that many more of the parameters is more or equally significant to the lenght, and that the width is in fact statistically insignificant. This means that we can expect some errors when prediciting with this model. 

```{r}
library(tree)
tree.logprice = tree(logprice~logcarat+cut+clarity+color+depth+table+xx+yy+zz, data=dtrain)
plot(tree.logprice)
text(tree.logprice, pretty=1)

```

**Q14:**    
```{r}
tree_pred = predict(tree.logprice, newdata=dtest)
mse_tree=mean((tree_pred - dtest$logprice)^2)
cat("The MSE of the regression tree is", mse_tree)

```


**Q15:**  When we average over different observations, the variance is typically lower than for one single observation. One can use the bootstrap to find $B$ repeated samples from the training data set, train each of the $B$ sets, and then average on the trained models. This is called bootstrap aggregation, or bagging, and will reduce the variance of the model. 

Bagging can typically be used in regression trees, since using one single tree will result in high variance. Then we get $B$ bootstrapped trees that we use for the final result by taking the average. Random forest is similar to bagging, but instead of using each of the predictors when choosing a split, a smaller sample of the predictors is used. This is called decorrelation of the trees, and will lead to an even lower variance.


**Q16:** In random forest one will have to choose the number of trees, $B$. A large number of trees will not lead to overfitting, and therefore $B$ is chosen sufficiently large, so that the error rate stabilizes. 

The other parameter that needs to be set is the number of predictors used in each split, $m$. One typically chooses $m \approx \sqrt{p}$, where $p$ is the number of predictors. The algorithm is no longer allowed to choose splits based on the majority of the predictors. Then if one predictor is particulary strong, it will not be at the top of the trees as often as it would if bagging was used. Hence, the other predictors are chosen more often, and the trees are thus decorrelated. 

**Q17:** The main difference between random forest and boosting is that in random forest, each tree is constructed based on the bootstrapped sample from the training data, while in boosting, the trees are build using the residuals, meaning that each tree is build using information from the previously built trees. This makes boosting an approach that learns slowly, which in general usually means that it performs well. 

**Q18:** When using random forest, one usually choose $m \approx \sqrt{p}$ as stated above, which in this case means that $ m \approx \sqrt{9} = 3$. Common choices for the number of trees are $B=300$ and $B=500$, and here $B=300$ should be sufficient. 

```{r random forest 1}
set.seed(300)
library(randomForest)
rf.logprice=randomForest(logprice~logcarat+cut+clarity+color+depth+table+xx+yy+zz, data=dtrain, mtry=3, ntree=300, importance=TRUE)
plot(rf.logprice)
```

**Q19:** We make a variable importance plot to study which of the covariates that are most important when explaining the price of a diamond. 
```{r}
varImpPlot(rf.logprice)
rf.pred=predict(rf.logprice, newdata=dtest)
mse_rf=mean((rf.pred-dtest$logprice)^2)
cat("The MSE of the random forest:",mse_rf)

```
We see that `yy` is the most important covariate when looking at node putiry, while `cut` seems to be the least important. However, when we look at the MSE it seems like `clarity` is the most important covariate and that `zz` is less important. 

**Q20:** Finally, we compare the results from c (subset selection), d (lasso), e (tree) and f (random forest): Which method has given the best performance on the test set and which method has given you the best insight into the relationship between the price and the covariates?
```{r}
library(utils)
library(pracma)

pred_bsm = predict(fit_bsm, newdata=dtest)
mse_bsm = mean((dtest$logprice - pred_bsm)^2)

mses=c(mse_bsm, mse_lasso, mse_tree, mse_rf)
names=c("Subset selection", "Lasso", "Tree", "Random forest")
mse_range = linspace(0,0.2,101)
plot(mses, axes=FALSE, xlab = 'Methods', ylab = 'MSE')
axis(1, at=1:4, lab=names)
axis(2,mse_range)
box()

print("the MSEs: ")
for (i in c(1,2,3,4)){
  print(paste(names[i], sep = ": ", round(mses[i], 4)))
}

```

We see that the regression tree has a substantially higher MSE than the rest, and that the random forest has the best MSE. This also indicats that the regression tree will provide us with the least information about the relation between the covariates and the price. This coincides with what we have previously observed, where the regression tree only takes two covariates into account in the prediction. 

Even though the remaining models have approximately equal MSE, some of them gives easier insight into the relationship between the covariates and the price. The random forest method for instance, operates as a black box, and thus gives little information about the effect of the different covariates on the response value. 

In both the subset selection method and the lasso method, the value of the regression parameters and which covariates are included in the resulting model indicates how the different covariates influences the response. The subset selection model gives a clear result of which covariates are considered significant, which provides an easily interpretable model. For instance, one can quickly conclude on which covariates does not have a high influence on the price by looking at the result from the subset selection. 

The lasso regression, in addition to removing the completely insignificant covariates, also shrinks the value of the coefficiants of the less significant covariates. This means that one can get an impression of what covariate has the most influence on the response variable as well. In our case, we see that the `logcarat` coefficient is substantially higher than the rest. This coincides well with our initial interpretation that the carat is the one covariate that influences the price the most. However, one should notice that this method of interpreting the model is not always correct, as regression coefficient also takes the value size of the covariates into account, meaning that covariates that contribute with lower values might have coefficients of higher values without being more significant.


# Problem 2: Unsupervised learning [3 points]

**Q21:** The principal component score is a measurement of how important a given set of observations is for a principal component. The principal components are the eigenvectors of the covariance matrix ${\hat {\bf R}}$, and thus a principal component score for a set of observations is the scaled observations multiplied with a principal component.
$$\text{score}_i = \sum_{j=1}^{p}x_{ij}\cdot(PCi_j)$$
```{r}
library(ElemStatLearn)
X=t(nci) #n times p matrix
ngroups=length(table(rownames(X)))
cols=rainbow(ngroups)
cols[c(4,5,7,8,19)] = "black"
pch.vec = rep(4,14)
pch.vec[c(4,5,7,8,14)] = 15:19

colsvsnames=cbind(cols,sort(unique(rownames(X))))
colsamples=cols[match(rownames(X),colsvsnames[,2])] 
pchvsnames=cbind(pch.vec,sort(unique(rownames(X))))
pchsamples=pch.vec[match(rownames(X),pchvsnames[,2])]

Z=scale(X)

pca=prcomp(Z)
plot(pca$x[,1],pca$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)


plot(1:dim(X)[2],pca$rotation[,1],type="l",xlab="genes",ylab="weight",main="First eigenvector")

```


**Q22:**  The plot "First eigenvector" shows the weights of the first eigenvector of $\hat{\bf{R}}$ which is also the first principal component. The reason why there is only $64$ principal component is that there can not be more principal components than the rank of $\bf{Z}$. This is because all the principal components are linearly independent, and thus the number of principal components (which is the eigenvectors of $\hat{\bf{R}}$) is bounded by the rank of $\bf{Z}$ which is $min(n, p) = n$. Because we have done a centering we do not have full rank anymore because the centering has to lead to one of the rows being linearly dependent on the others. The rank of $\bf{Z}$ is then $n-1=63$ and it should thus have been $63$ principal components. When we study the last principal component we see that it is almost zero and the reason why it is not equal to zero is because of rounding error in the first $63$ principal components. A principal component score corresponds to a principal component and it thus follows that we have $64$ component scores as well. 
  

**Q23:** The proportion of variance explained by a single principle component is calculated as 
$$\text{PVE}=\frac{\text{variance explained by }m\text{th component}}{\text{total variance}} = \frac{\frac{1}{n}\sum_{i=1}^{n}(\sum_{j = 1}^{p} \phi_{jm}z_{ij})^2}{\frac{1}{n}\sum_{j=1}^{p}\sum_{i=1}^{n}z_{ij}}.$$
From this formula it is also easy to see that the total variance is `sum(pca$sdev^2)=p`. The total variance defined above is the same as the sum of the variances of the columns of $\bf{Z}$, which is $p$ since they are standardized, and thus every column has variance $1^2=1$. An easier way of computing the proportion of variance is to use the eigenvalues corresponding to the principle components. It can be shown that $\text{Var}(\phi_j) = \lambda_j$ and thus $$\text{PVE}=\frac{\lambda_m}{\sum_{j=1}^{p}\lambda_j}$$

```{r proportion of variance plot}
pve=100*pca$sdev^2/sum(pca$sdev^2)
plot(cumsum(pve), type = "o", ylab = "Cumulative PVE", xlab = "Principal component", col="brown3")

```
We can see from the plot that it takes about 30 principal components to explain 80% of the variance, we check for the different number of principal components to check this and find the exact answer.
```{r proportion of variance}

totvar = sum(pca$sdev^2)
pve32=sum(pca$sdev[1:32]^2)/totvar

pve31 = sum(pca$sdev[1:31]^2)/totvar

cat("The first 31 principal components explains", pve31 * 100, "% of the variance and 32 principal components explains", pve32 * 100, "% of the variance. Thus we need 32 principal components to explain 80% of the variance in the data.")
```

**Q24**: From the plot above, plotting PC1 agianst PC2, the most obvious clustering is the blue clustering, where the genes are from `MELANOMA` cancer tumors. The green `LEUKEMIA` genes are also easy to seperate from the others, and so are genes from `COLON` cancer tumors. We observe that the genes from other cancer types are mostly located in the left upmost corner and are also, to some degree, clustered. The exeption is the genes from the breast cancer tumor, which are not clustered at all and are spread around the entire plot. 
  It is known that the `K562` samples are leukemia cells and we can see from the plot that they are clustered together with the other leukemia samples. It is harder to say something about the breast cancer samples, `MCF7`, because the other breast cancer samples are also spread widely around. The unknown sample lays in the upper left corner, so it could belong to any of those groups. However, the most likely cancer types are `NSCLC` or `OVARIAN` because the closest points are of those types.

We plot a couple of other projections of the data onto the principal components to see if we can find other interesting relationships. It is mostly interesting to look at the first principal comonents, because those contain the most variance. It could have been interesting to find plots where the breast cancer cells are clustered together or find a relation for the unknown sample, but this turned out to be difficult. Below two other pairs of principal components are plotted together. In the first plot we see PC1 agianst PC3 and here we also see that green and yellow are clustered. Again, the remaining cancer types are slightly clustered and it is hard to find a relation between the cases of breast cancer. However we see that the `MFC7` samples of breast cancer are slightly more separated from the cluster of `COLON` cases and are still closely associated with a breast cancer case. This is shown even more clearly by plotting PC3 against PC4, as shown below. The `UNKNOWN` sample was hard to classify for all combinations we tried, and because of this we conclude that it probably belongs to one of the cancer types that are difficult to separate from the others. Our strongest hypothesis is that it belongs to the `NSCLC` type.



```{r}
plot(pca$x[,1],pca$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)

plot(pca$x[,1],pca$x[,3],xlab="PC1",ylab="PC3",pch=pchsamples,col=colsamples)
legend("bottomleft",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)

plot(pca$x[,3],pca$x[,4],xlab="PC3",ylab="PC4",pch=pchsamples,col=colsamples)
legend("bottomleft",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
```


**Q25:**: A hierarchical clustering algorithm starts with dividing all elements into it's own cluster and iteratively merges the closest clusters to bigger clusters. It then has to be decided how one can say which clusters are nearer than other when the dimensions grow larger than two. The point's positions are defined by a vector of the values of the covariates and thus we can use a norm to express the distance between two points. The most common norm is the 2-norm (also called Eucledian distance) and for deciding the distance between two clusters there is also several choices. There are 3 common methods, where average linkage is calculating the mean position of each cluster and calculating the distance between these two points.

**Q26:**: Perform hierarchical clustering with Euclidean distance and average linkage on the scaled gene expression in `Z`. Observe where our samples labelled as K562, MCF7 and `UNKNOWN` are placed in the dendrogram. Which conclusions can you draw from this?
```{r q26}
library(ggplot2)
library(ggdendro)

ggdendrogram(hclust(dist(Z), method = "average"))+labs(title="Hierarchical clustering")

```
From the Hierarchical clustering performed above we can observe how the algorithm places the samples compared to the well-known samples. We start by looking at the `K562` samples and see that they are clustered together with `LEUKEMIA` as expected. The Hierarchical clustering, as we did in **Q24**, struggles with clustering the `BREAST` samples together, but in the same way we saw it classifies the `MCF7` samples closest to two `BREAST` samples. The `UNKNOWN` sample is by the algorithm nearest to `OVERIAN` samples and not `NSCLC` as we suspected. But we do get confirmed that `NSCLC` is somewhat randomly spread, so the basis of our suspicion is well-grounded, we just did not belive that the sample was closer to the `OVERIAN` samples.

**Q27:**: In the plot below it is preformed hierarchical clustering based on the principal components. This means that the positions of the points are based on the values when the vector are multiplied with the principal components. This means that the clusterings are based on vectors of length 64 in stead of 6830. One entry of the heat map grid shows the value of a PCA multiplied with the gene composition vector of a sample. The horizontal axis is the PCAs sorted by the amount of variation explained in the data, from most to less. On the vertical axis the covariates are sorted by the hierchial clustering algorithm.

```{r}
library(pheatmap)
npcs=64 
pheatmap(pca$x[,1:npcs],scale="none",cluster_col=FALSE,cluster_row=TRUE,clustering_distance_rows = "euclidean",clustering_method="average",fontsize_row=5,fontsize_col=5)
```




# Problem 3: Flying solo with diabetes data [6 points]

```{r}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest
```

**Q28:** We start by getting to know the _training data_, by producing summaries and plots. Write a few sentences about what you observe and include your top 3 informative plots and/or outputs.
```{r}

flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest

library(GGally)
summary(ctrain) #gir info om vanlige verdier for kovariatene
str(ctrain) #en kompkt måte å se hvordan datasettet ser ut 
ctrain$diabetes = as.factor(ctrain$diabetes)
ggpairs(ctrain, aes(color=diabetes))
```
Firstly, to get an impression of how the values of the covariates are, we print the summary of the data frame. This can be useful for instance if we wanted to perform regression on the data set later, because it would make it easier to interpret the regression coefficients. The summary for instance tells us that the data set only includes persons of the age from 21 to 70, and that the median value of `ped` is 0.4, which people outside the medical profession probably do no know beforehand. This knowledge can for instance help preventing interpreting larger regression coefficient values for `ped` as that coefficiant being more significant, as it might just be an adjustment for the `ped` values being lower in general.  
 
Secondly, we have included a pair plot to investigate how the different covariates influences the response variable, and also how they relate to each other. In an ideal model, all of the covariates are independent, to avoid multicollinearity, and so one should check whether this is in fact true for our model. We see from the correlation plots that most of the covariates have low correlation, with the exeption of `skin` and `bmi`, and `npreg` and `age`.  
The plots on the diagonal show the frequency of the different covariate values for people with and without diabetes. Looking at these plots give a good indication of whether the specific covariate influence whether or not a person has diabetes. If the diabetes and non-diabetes sections does not overlap, this indicates that the covariate has an effect on the response. From our plot, we can then say that we expect that `npreg`, `glu`, `ped` and `age` are the covariates that influence whether a person has diabetes the most.  
 

**Q29:** Use different methods to analyse the data. In particular use 

* one method from Module 4: Classification   
We choose logistic regression since we only have two responce classes. When using logistic regression, the model is based on probabilities that $Y=1$. That means the probability that a person has diabetes. In this case, this probability is calculated by 
$$ p(x_i)= p_i = \frac{e^{{\beta}_{intercept} + {\beta}_{npreg}x_{npreg,i} + {\beta}_{glu}x_{glu,i} + {\beta}_{bp}x_{bp,i} + {\beta}_{skin}x_{skin,i} + {\beta}_{bmi}x_{bmi,i} + {\beta}_{ped}x_{ped,i} + {\beta}_{age}x_{age,i}}}
{1+e^{{\beta}_{intercept} + {\beta}_{npreg}x_{npreg,i} + {\beta}_{glu}x_{glu,i} + {\beta}_{bp}x_{bp,i} + {\beta}_{skin}x_{skin,i} + {\beta}_{bmi}x_{bmi,i} + {\beta}_{ped}x_{ped,i} + {\beta}_{age}x_{age,i}}}.$$ 
One can also write this as 
$$ log \frac{p_i}{1-p_i} = {{\beta}_{intercept} + {\beta}_{npreg}x_{npreg,i} + {\beta}_{glu}x_{glu,i} + {\beta}_{bp}x_{bp,i} + {\beta}_{skin}x_{skin,i} + {\beta}_{bmi}x_{bmi,i} + {\beta}_{ped}x_{ped,i} + {\beta}_{age}x_{age,i}}.$$

These coefficients are found using maximun likelihood estimation. In logistic regression we assume that the observations are independent of each other, and the variables should not either be to highly correlated. As seen when plotting the variables with each other, one can observe some mutlicollinearity between `skin` and `bmi` as well as between `npreg` and `age`. The log-odds is the left side of the last equation. We require little or no linearity between the log-odds and the independent variables, and we also assume that we have a large sample size.    

Fitting the model:
```{r logistic regression}
lrtrain=ctrain
lrtest=ctest
library(glmnet)
diab.glm = glm(diabetes ~ ., data = ctrain, family = "binomial")
summary(diab.glm)
```

If we look at the summary of the model, we see that there are three significate covariates with p-value less than $0.05$. These are `glu`, `bmi` and `ped`, which in fact all have p-values less than $0.01$. The covariates `bp` and `skin` have rather high p-values, which means that they may not be that important in the model. 


If one appiles this model to the test data one can find the proporsion of mistakes that are made, called the misclassification test error rate.
```{r log reg eval}
diab.glm_prob=predict(diab.glm, newdata=lrtest, type="response")
diab.glm_pred=ifelse(diab.glm_prob > 0.5, 1, 0)
conf_test=table(diab.glm_pred, lrtest$diabetes)
conf_test
misclass_test = (232-sum(diag(conf_test)))/232
misclass_test
cat("The misclassification error for the test data is", misclass_test)
```



Finding ROC-curve and AUC
```{r}
library(pROC)
par(pty='s')
glm_ROC = roc(lrtest[ ,1], diab.glm_prob, plot=TRUE, legacy.axces=TRUE)
glm_ROC
```

The area under the ROC curve, hence the AUC, is calcualed to be 0.8451







* one method from Module 8: Trees (and forests)

We choose to use bagging to analyse the data. When using bagging one constucts bootstrap samples, and here we make $500$ samples. A classification tree is made from each sample, $\hat{f}^{*b}$. $b=1...B$, where $B$ is the number of bootstrap samples. Then one avareage over these predictions. Each of these classification trees will have high variance and low bias, but when we average over the trees, the variance decreases. We assume that the observations are independent.     

When using bagging instead of one simple classification tree, the variance is decreased, but this is on the cost of interpretability. When you average over multiple trees, you loose some of the interpretability that we find in classification trees. However it is possible to make varibale importance plots. Here the predictors are sorted based on their importance, where the more important predictors for either increase in MSE of node purity are placed further up than the less important ones. 

```{r bagging}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest

library(randomForest)
set.seed(2)
diab.bagging=randomForest(diabetes ~ ., data = ctrain, mtry=7, ntree=500, importance=TRUE)
plot(diab.bagging)
summary(diab.bagging)

```

The number of bootstrap samples, $B$, is chosen such that the error has stabilised, and is often chosen to $300$ or $500$. We se that the error is clearly stabilised at $B=500$, and choose that to be the number of bootstrap samples. 

```{r}
varImpPlot(diab.bagging)

```
In the variance importance plot we see that `glu` is the most important covariate both when it comes to increase in MSE when assigning random variables instead of the covariate and in increase of the Gini index when using thw covariate. One can also see that `npreg` and `bp` are the least important variables. 


Misclassification rate for the test data
```{r}
diab.bag_prob=predict(diab.bagging, newdata=ctest, type="response")
diab.bag_pred=ifelse(diab.bag_prob>0.5, 1, 0)
conf_test_bagging=table(diab.bag_pred, ctest$diabetes)
conf_test_bagging
misclass_test_bagging = (232-sum(diag(conf_test_bagging)))/232
cat("The misclassification rate for the test data is", misclass_test_bagging)
```


ROC curve and AUC
```{r ROC bagging}
library(pROC)
par(pty='s')
bagging_ROC=roc(ctest[ ,1], diab.bag_prob, plot=TRUE, legacy.axes=TRUE)
bagging_ROC

```
The $AUC$ is $0.8288$. 

* one method from Module 9: Support vector machines

We choose the support vector machines method with a radial kernel. The radial kernel is chosen because it is efficient and helps avoid overfitting of the data. The radial kernel is defined as 
$$ K(\textbf{x}_i, \textbf{x}_i') = \text{exp}(- \gamma \sum_{j = 1}^{p}(x_{ij} - x_{i'j})^2),  $$ 
where $\gamma$ is a tuning parameter and $p$ is the number of covariates. We find the optimal values for the cost of violating the margin and $\gamma$ by the function tune() in R. This results in a model where the width of the margin is $0.1$, $\gamma=0.001$ and $\cost=5$ as seen from the summary of the selected model.  
 
We notice that the model uses a quite large number of support vectors, 188 out of 300 possible. This could give us suspicions that the model is overfitted, but since the misclassification error is still rather low, it does not seem to be a substantial problem. 
 
```{r svm}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest
svmtrain=ctrain
svmtest=ctest

library(e1071)
set.seed(2)
CV_radial = tune(svm, factor(diabetes) ~ ., data=ctrain, kernel="radial", ranges=list(gamma=c(10^-5, 10^-4,0.001,0.01,0.1,1,2,3,4,5), cost=c(0.001, 0.01, 0.1, 1, 5, 10, 20,40,80,100,120,140,160)))
svm_best_model=CV_radial$best.model
summary(svm_best_model)

#Finding miclassification rate for the test set:
svm_pred_test=predict(svm_best_model, newdata=svmtest, type="response")
conf_test_svm=table(svm_pred_test, ctest$diabetes)
conf_test_svm
misclas_test_svm=(232-sum(diag(conf_test_svm)))/232
cat("Misclassification rate for the test data:", misclas_test_svm)

par(pty='s')
svm_ROC=roc(ctest[ ,1], svm_prob_test, plot=TRUE, legacy.axes=TRUE)
svm_ROC
```


* one method from Module 11: Neural networks
We now use a machine learning technique based on neural networks. Because the application is classification with only two classes we use a network with only one output node. The input layer consists of one node for each covariate which gives an input layer with a total of 7 nodes. We use a method in R called `nnet` which uses one hidden layer where the number of hidden nodes can be varied to fit our problem. To see what works best in our case, we will use cross validation to find the best choice for this tuning parameter. 

```{r neural networks}
#loading again because the data has to be unchanged
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest

nntrain <- ctrain
nntest <- ctest

mean <- apply(ctrain, 2, mean)
mean[1] = 0
std <- apply(ctrain, 2, sd)
std[1] = 1
nn_train <- scale(nntrain, center=mean, scale=std)
nn_test <- scale(nntest, center=mean, scale=std)

nn_train.df <- as.data.frame(nn_train)
nn_test.df <- as.data.frame(nn_test)

library(nnet)
library(NeuralNetTools)
library(pROC)

#Cross validation to find best size of hidden layer
set.seed(2)
cv_nn = tune(nnet, diabetes ~ ., data=nn_train.df, ranges=list(size=1:10), linout=FALSE, entropy=TRUE)
nn_best_model=cv_nn$best.model

nn_prob_test=predict(nn_best_model, newdata=nn_test) #jeg får ikke til , type="response"
#nn_prob_test
summary(nn_prob_test)
nn_pred_test=ifelse(nn_prob_test>0.5, 1, 0)
conf_test_nn=table(nn_pred_test, ctest$diabetes)
conf_test_nn
misclas_test_nn=(length(ctest[, 1])-sum(diag(conf_test_nn)))/length(ctest[, 1])
cat("Misclassification rate for the test data: ", misclas_test_nn)

roc_curve = roc(response=ctest[,1], predictor=nn_prob_test, legacy.axes=TRUE)

rel_imp <- garson(nn_best_model, bar_plot = FALSE)$rel_imp
cols <- colorRampPalette(c( 'lightgreen', 'darkgreen'))(7)[rank(rel_imp)]
plotnet(nn_best_model, circle_col = list(cols, 'lightblue'))

ctrain[1,]


```
We see from the results that the size of the hidden layer is set to only one node (the visualization is shown above) and the misclassification rate is 19,8% for the fitted model. This means that a quite small model was enough to predict as well as earlier models. To interpret the model we look at the relative importance of the imput nodes and see, as we did before, that the covariates `glu`, `bmi`, `ped` and `age` are the most important covariates (the darkest green imply most important). Another notable fact is that all covariates are used, so everyone is considered significant enough to be included. If not a neural net would have set the weight to zero.  

```{r ROC neural nets} 
par(pty='s')
roc_curve = roc(response=ctest[,1], predictor=nn_prob_test, legacy.axes=TRUE, plot=TRUE)
cat("AUC: Area under curve:", roc_curve$auc)

```
The neural net that where trained to solve the problem worked over all very well. The misclassification rate where as good as in the earlier methods and it chose a small model with only one hidden node that also made it easier to understand the model. We also see from the ROC-curve and AUC that it looks good and preforms better than random guessing.
**Q30:** Conclude with choosing a winning method, and explain why you mean that this is the winning method.
We now compare the MSEs of the different methods to see which is the preferable. 

```{r}
mses=c(misclass_test, misclass_test_bagging, misclas_test_svm, misclas_test_nn)
names=c("Logistic regression", "Bagging", "Support vector machine", "Neural network")
mis_class_range = linspace(0,0.4,101) #Endre denne?
plot(mses, axes=FALSE, xlab = 'Methods', ylab = 'Misclassification rate')
axis(1, at=1:4, lab=names)
axis(2,mis_class_range)
box()

print("The misclassification rates: ")
for (i in c(1,2,3,4)){
  print(paste(names[i], sep = ": ", round(mses[i], 4)))
}
```
We see that the logistic regression has the lowest misclassification rate, which is a good indication that this is the best method for our problem. 


